# Research Proposal Outline

## 1. Project Title  
**Exploring the Role of Large Language Models (LLMs) in Automating Risk Management Framework (RMF) Assessment and Authorization for DoD Networked Systems**

---

## 2. Significance / Contribution to the Discipline  
- The Department of Defense (DoD) relies heavily on the RMF process for cybersecurity authorization.  
- The RMF process is resource-intensive and time-consuming, often delaying innovation and increasing compliance burdens.  
- Emerging automation trends—such as Infrastructure as Code (IaC) and Security as Code—demonstrate potential for scalable, verifiable, and repeatable compliance workflows.  
- This research explores the emerging application of LLMs (e.g., GPT-4) in automating RMF documentation, control analysis, and validation tasks, in parallel with other machine-readable frameworks like IaC.  
- It contributes to cybersecurity and AI governance literature by evaluating the feasibility, risks, and opportunities for LLM integration in regulated environments.

---

## 3. Research Question  
**To what extent can Large Language Models (LLMs) support the automation of Risk Management Framework (RMF) assessment and authorization processes for networked systems within the Department of Defense, and what are the practical implications for accuracy, compliance, and trust?**

---

## 4. Aims and Objectives  
**Aim:**  
To assess the applicability, effectiveness, and challenges of integrating LLMs into DoD RMF processes for cybersecurity risk management.

**Objectives:**  
- To examine current use cases or pilots of LLMs in RMF or similar compliance environments.  
- To evaluate LLM capabilities in generating and validating RMF documentation (e.g., SSPs, POA&Ms).  
- To compare the role of LLMs with parallel automation trends like Infrastructure as Code in system validation.  
- To identify the operational, legal, and ethical challenges of adopting LLMs in DoD cybersecurity risk workflows.  
- To suggest guidelines or criteria for future deployment of LLMs in RMF-based systems.

---

## 5. Key Literature  
- Correa et al. (2023) – global ethics in AI governance
  - https://pubmed.ncbi.nlm.nih.gov/37876898/ 
- NIST 800-37 and 800-53 – core RMF documentation
    - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-37r2.pdf
    - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf 
- OSCAL (NIST) – machine-readable security controls framework
  - https://pages.nist.gov/OSCAL/learn/
- Özdoğan, Erdal & Ceran, Onur & ÜSTÜNDAĞ, Mutlu. (2023) - Systematic Analysis of Infrastructure as Code Technologies 
 - https://www.researchgate.net/publication/376476103_Systematic_Analysis_of_Infrastructure_as_Code_Technologies
---

## 6. Methodology / Research Design  
This research will use an Exploratory Research Design with primarily Qualitative Methods. The chosen methodology aligns with the project’s goal of understanding emerging applications of Large Language Models (LLMs) in automating the Risk Management Framework (RMF) assessment and authorization processes within the Department of Defense context—an area with limited existing formal studies.

The main qualitative techniques employed will include:

Case Studies:
- Conducting in-depth analyses of pilot projects, industry reports, governmental documents (such as DoD CIO guidance, NIST publications), and case studies from organizations that have implemented similar technologies. This approach will help identify practical examples, lessons learned, and documented outcomes related to LLM integration.

Semi-structured Interviews:
- Conducting expert interviews with subject matter specialists in cybersecurity compliance, RMF processes, government cybersecurity policy, and artificial intelligence integration. Interviews will provide nuanced insights into operational feasibility, anticipated barriers, ethical concerns, and practical recommendations for adoption.

Secondary Research:
- Reviewing and synthesizing existing literature, such as NIST Special Publications (800-37 and 800-53), OSCAL documentation, DoD policy memos, and academic literature on automation and AI governance, to contextualize findings within established frameworks and guidelines.

The research will utilize an inductive approach to data analysis, allowing themes, patterns, and insights to emerge organically from the collected qualitative data. This methodology is ideal for understanding the complexities and implications of LLM adoption within the highly regulated DoD RMF environment.

Where relevant and feasible, mixed-method elements may be integrated later to quantitatively compare practical outcomes (such as time savings or accuracy improvements). However, the primary emphasis will remain qualitative due to the exploratory nature of the topic.

---

## 7. Ethical Considerations and Risk Assessment  
- This research does not involve human subjects or sensitive system data directly.  
- Ethical considerations include avoiding bias in AI model evaluation and responsibly handling proprietary or policy-restricted materials.  
- Risks include access limitations to real-world DoD implementations; mitigated through publicly available documents and de-identified case studies.

---

## 8. Artefact Description  
- The primary artefact will be a comprehensive analytical report and a visual framework for evaluating LLM suitability in RMF workflows.  
- If possible, the research will include a conceptual model comparing LLM-augmented workflows to OSCAL/IaC-based pipelines for compliance automation.

---

## 9. Timeline of Proposed Activities

TBD

# Notes/Activites:

Exploratory Research is the most suitable design for this project. The research investigates a relatively new application—using Large Language Models (LLMs) for RMF automation in the DoD context—where the boundaries and best practices are not yet clearly defined. I'm aiming to gain a better understanding of feasibility, use cases, risks, and opportunities, rather than to test a specific hypothesis or quantify relationships.


Given the exploratory nature and the emphasis on emerging applications and feasibility, qualitative methods are the most appropriate. Specifically, you might consider:

- Case Studies: Reviewing pilot projects, government publications, or corporate case studies where LLMs or automation were applied to compliance or cybersecurity tasks.
- Interviews: Semi-structured interviews with subject matter experts in RMF, cybersecurity compliance, and AI integration within government or defense settings.
- Secondary Research: Using existing academic and policy literature (e.g., NIST, DoD CIO memos, OSCAL frameworks) to support your analysis.

A Mixed Methods approach could also be valid if I later incorporate small-scale quantitative comparisons (e.g., comparing document generation times or accuracy rates between LLMs and traditional methods).

To effectively carry out this project, I will need or need to strengthen the following skills:
- Literature Review and Qualitative Analysis: To critically synthesize existing research and evaluate use cases.
- Interview Design and Thematic Coding: For conducting and analyzing expert interviews, if used.
- AI and RMF Technical Understanding: A solid grasp of how LLMs work, how RMF operates (including NIST 800-37/53), and how automation frameworks like OSCAL and IaC function.
- Data Interpretation and Synthesis: To draw meaningful insights from varied and possibly unstructured information.
- Ethical Awareness in AI and Research: Especially regarding bias, governance, and DoD compliance standards.


Need to consider Interviews, Survey Methods, and Questionnaire Design
